## 3.1 Data-Parallel Architectures  数据并行架构

Various strategies are used by different processor architectures to avoid stalls. A CPU is optimized to handle a wide variety of data structures and large code bases. CPUs can have multiple processors, but each runs code in a mostly serial fashion, limited SIMD vector processing being the minor exception. To minimize the effect of latency, much of a CPU’s chip consists of fast local caches, memory that is filled with data likely to be needed next. CPUs also avoid stalls by using clever techniques such as branch prediction, instruction reordering, register renaming, and cache prefetching [715]

不同的处理器架构使用各种策略来避免停顿。 CPU 经过优化以处理各种数据结构和大型代码库。 CPU 可以有多个处理器，但每个处理器都以串行方式运行代码，有限的 SIMD 向量处理是一个小例外。 为了最大限度地减少延迟的影响，CPU 的大部分芯片都由快速本地缓存(fast local caches)组成，内存中充满了接下来可能需要的数据。 CPU 还通过使用分支预测(branch prediction)、指令重新排序(instruction reordering)、寄存器重命名(register renaming)和缓存预取(cache prefetching)等巧妙技术来避免停顿 [715]

GPUs take a different approach. Much of a GPU’s chip area is dedicated to a large set of processors, called shader cores, often numbering in the thousands. The GPU is a stream processor, in which ordered sets of similar data are processed in turn. Because of this similarity—a set of vertices or pixels, for example—the GPU can process these data in a massively parallel fashion. One other important element is that these invocations are as independent as possible, such that they have no need for information from neighboring invocations and do not share writable memory locations. This rule is sometimes broken to allow new and useful functionality, but such exceptions come at a price of potential delays, as one processor may wait on another processor to finish its work.

GPU 采用不同的方法。 GPU 的大部分芯片区域专用于大量处理器，称为着色器核心，通常有数千个。 GPU 是一个流处理器，其中轮流处理相似数据的有序集合。 由于这种相似性——例如一组顶点或像素——GPU 可以以大规模并行方式处理这些数据。 另一个重要因素是这些调用尽可能独立，这样它们就不需要来自相邻调用(neighboring invocations)的信息并且不共享可写内存位置(writable memory locations)。 有时会打破此规则以允许新的有用的功能，但这个例外是以潜在延迟为代价的，因为一个处理器可能会等待另一个处理器完成其工作。

The GPU is optimized for throughput, defined as the maximum rate at which data can be processed. However, this rapid processing has a cost. With less chip area dedicated to cache memory and control logic, latency for each shader core is generally considerably higher than what a CPU processor encounters [462].

GPU 针对吞吐量进行了优化，吞吐量定义为可以处理数据的最大速率。 然而，这种快速处理是有代价的。 由于专用于高速缓存和控制逻辑的芯片区域较少，每个着色器核心的延迟通常比 CPU 处理器遇到的延迟高得多 [462]。

Say a mesh is rasterized and two thousand pixels have fragments to be processed; a pixel shader program is to be invoked two thousand times. Imagine there is only a single shader processor, the world’s weakest GPU. It starts to execute the shader program for the first fragment of the two thousand. The shader processor performs a few arithmetic operations on values in registers. Registers are local and quick to access, so no stall occurs. The shader processor then comes to an instruction such as a texture access; e.g., for a given surface location the program needs to know the pixel color of the image applied to the mesh. A texture is an entirely separate resource, not a part of the pixel program’s local memory, and texture access can be somewhat involved. A memory fetch can take hundreds to thousands of clock cycles, during which time the GPU processor is doing nothing. At this point the shader processor would stall, waiting for the texture’s color value to be returned.

假设一个网格(mesh)被光栅化，两千个像素有片段需要被处理； 像素着色器程序将被调用两千次。 想象一下只有一个着色器处理器，世界上最弱的 GPU。 它开始为 2000 的第一个片段执行着色器程序。 着色器处理器对寄存器中的值执行一些算术运算。 寄存器是本地的并且可以快速访问，因此不会发生停顿。 然后着色器处理器开始执行诸如纹理访问之类的指令； 例如，对于给定的表面位置，程序需要知道应用于网格(mesh)的图像的像素颜色。 纹理是一个完全独立的资源，而不是像素程序本地内存的一部分，并且可能涉及到纹理访问。 内存获取(memory fetch)可能需要耗费数百到数千个时钟周期，在此期间 GPU 处理器什么都不做。 此时着色器处理器会停止，等待返回纹理的颜色值。

To make this terrible GPU into something considerably better, give each fragment a little storage space for its local registers. Now, instead of stalling on a texture fetch, the shader processor is allowed to switch and execute another fragment, number two of two thousand. This switch is extremely fast, nothing in the first or second fragment is affected other than noting which instruction was executing on the first. Now the second fragment is executed. Same as with the first, a few arithmetic functions are performed, then a texture fetch is again encountered. The shader core now switches to another fragment, number three. Eventually all two thousand fragments are processed in this way. At this point the shader processor returns to fragment number one. By this time the texture color has been fetched and is available for use, so the shader program can then continue executing. The processor proceeds in the same fashion until another instruction that is known to stall execution is encountered, or the program completes. A single fragment will take longer to execute than if the shader processor stayed focused on it, but overall execution time for the fragments as a whole is dramatically reduced.

为了让这个糟糕的 GPU 变得更好，给每个片段一个小的存储空间用于它的本地寄存器。 现在，着色器处理器可以切换并执行另一个片段，即 2000 中的第 2 个片段，而不是在纹理获取时停止。 这个切换非常快，除了注意到第一个正在执行的指令外，第一个或第二个片段中的任何内容都不会受到影响。 现在执行第二个片段。 与第一个相同，执行一些算术函数，然后再次遇到纹理获取(texture fetch)。 着色器核心现在切换到另一个片段，第三个。 最终两千个碎片都这样处理。 此时着色器处理器返回到第一个片段。 此时纹理颜色已获取并可供使用，因此着色器程序可以继续执行。 GPU中的处理器以相同的方式继续执行，直到遇到另一条已知会停止执行的指令，或者程序完成。上述方式与着色器处理器专注于处理单个片段的方式相比，在单个片段上的执行时间会更长，但所有片段的整体执行时间会大大减少。

In this architecture, latency is hidden by having the GPU stay busy by switching to another fragment. GPUs take this design a step further by separating the instruction execution logic from the data. Called single instruction, multiple data (SIMD), this arrangement executes the same command in lock-step on a fixed number of shader programs. The advantage of SIMD is that considerably less silicon (and power) needs to be dedicated to processing data and switching, compared to using an individual logic and dispatch unit to run each program. Translating our two-thousand fragment example into modern GPU terms, each pixel shader invocation for a fragment is called a thread. This type of thread is unlike a CPU thread. It consists of a bit of memory for the input values to the shader, along with any register space needed for the shader’s execution. Threads that use the same shader program are bundled into groups, called warps by NVIDIA and wavefronts by AMD. A warp/wavefront is scheduled for execution by some number GPU shader cores, anywhere from 8 to 64, using SIMD-processing. Each thread is mapped to a SIMD lane.

在这种架构中，通过切换到另一个片段让 GPU 保持忙碌来隐藏延迟。 GPU 通过将指令执行逻辑与数据分离，使这种设计更进一步发展。 称为单指令、多数据 (SIMD)，这种安排在固定数量的着色器程序上以锁步(lock-step)方式执行相同的命令。 SIMD 的优势在于，与使用单独的逻辑和调度单元(individual logic and dispatch unit)来运行每个程序相比，处理数据和切换所需的芯片（和功率）要少得多。 将我们的 2000 个片段示例转换为现代 GPU 术语，片段的每个像素着色器调用称为一个线程。 这种类型的线程不同于 CPU 线程。 它包含一些用于着色器输入值的内存，以及着色器执行所需的任何寄存器空间(register space)。 使用相同着色器程序的线程被捆绑成组，NVIDIA 称为 warp，AMD 称为 wavefronts。 一个 warp/wavefront 被安排由一定数量的 GPU 着色器核心执行，从 8 到 64 个，使用 SIMD 处理(SIMD-processing)。 每个线程都映射到一个 SIMD 通道。


Say we have two thousand threads to be executed. Warps on NVIDIA GPUs contain 32 threads. This yields 2000/32 = 62.5 warps, which means that 63 warps are allocated, one warp being half empty. A warp’s execution is similar to our single GPU processor example. The shader program is executed in lock-step on all 32 processors. When a memory fetch is encountered, all threads encounter it at the same time, because the same instruction is executed for all. The fetch signals that this warp of threads will stall, all waiting for their (different) results. Instead of stalling, the warp is swapped out for a different warp of 32 threads, which is then executed by the 32 cores. This swapping is just as fast as with our single processor system, as no data within each thread is touched when a warp is swapped in or out. Each thread has its own registers, and each warp keeps track of which instruction it is executing. Swapping in a new warp is just a matter of pointing the set of cores at a different set of threads to execute; there is no other overhead. Warps execute or swap out until all are completed. See Figure 3.1.

假设我们有两千个线程要执行。 NVIDIA GPU 上的warps包含 32 个线程。 这会产生 2000/32 = 62.5 个 warp，这意味着分配了 63 个 warp，其中一个 warp 有一半是空的。 warp 的执行类似于我们的单 GPU 处理器示例。 着色器程序在所有 32 个处理器上以锁步(lock-step)方式执行。 当遇到内存提取(memory fetch)时，所有线程都会同时遇到它，因为对所有线程都执行相同的指令。 fetch 信号表明这个线程的warp将停止，所有线程都在等待它们的（不同的）结果。 warp 不会停止，而是换出另一个 32 线程的 warp，然后由 32 个内核执行。 这种交换与我们的单处理器系统一样快，因为当一个 warp 被换入或换出时，每个线程内的数据都不会被触及(touched)。 每个线程都有自己的寄存器，每个 warp 跟踪它正在执行的指令。 换入一个新的 warp 只是将一组核心指向一组不同的线程来执行； 没有其他开销。 Warps执行或换出(swap out)直到全部完成。 见图 3.1。

![3.1](https://img-blog.csdnimg.cn/d6233a932dd249908cafb6f674ef74b1.png)


Figure 3.1. Simplified shader execution example. A triangle’s fragments, called threads, are gathered into warps. Each warp is shown as four threads but have 32 threads in reality. The shader program to be executed is five instructions long. The set of four GPU shader processors executes these instructions for the first warp until a stall condition is detected on the “txr” command, which needs time to fetch its data. The second warp is swapped in and the shader program’s first three instructions are applied to it, until a stall is again detected. After the third warp is swapped in and stalls, execution continues by swapping in the first warp and continuing execution. If its “txr” command’s data are not yet returned at this point, execution truly stalls until these data are available. Each warp finishes in turn.

图 3.1： 简化的着色器执行示例。 三角形的片段，称为线程，聚集成warps。 每个 warp 显示为四个线程，但实际上有 32 个线程。 要执行的着色器程序有五个指令长。 一组四个 GPU 着色器处理器为第一个 warp 执行这些指令，直到在“txr”命令上检测到停顿条件，这需要时间来获取其数据。 第二个 warp 被换入(swapped in)，着色器程序的前三个指令被应用到它，直到再次检测到停顿。 在第三个 warp 换入并停止后，执行通过换入(swap in)第一个 warp 并继续执行来继续。 如果此时尚未返回其“txr”命令的数据，则执行真正停止，直到这些数据可用。 每个warp依次结束。



In our simple example the latency of a memory fetch for a texture can cause a warp to swap out. In reality warps could be swapped out for shorter delays, since the cost of swapping is so low. There are several other techniques used to optimize execution [945], but warp-swapping is the major latency-hiding mechanism used by all GPUs. Several factors are involved in how efficiently this process works. For example, if there are few threads, then few warps can be created, making latency hiding problematic.

在我们的简单示例中，纹理的内存提取延迟会导致warp换出(swap out)。 实际上，warp 可以被以更短的延迟换出(swap out)，因为交换的成本很低。 还有其他几种技术可用于优化执行 [945]，但 warp-swapping 是所有 GPU 使用的主要延迟隐藏机制。 此过程的工作效率涉及多个因素。 例如，如果线程很少，则可以创建很少的warp，从而使延迟隐藏成为问题。


The shader program’s structure is an important characteristic that influences efficiency. A major factor is the amount of register use for each thread. In our example we assume that two thousand threads can all be resident on the GPU at one time. The more registers needed by the shader program associated with each thread, the fewer threads, and thus the fewer warps, can be resident in the GPU. A shortage of warps can mean that a stall cannot be mitigated by swapping. Warps that are resident are said to be “in flight,” and this number is called the occupancy. High occupancy means that there are many warps available for processing, so that idle processors are less likely. Low occupancy will often lead to poor performance. The frequency of memory fetches also affects how much latency hiding is needed. Lauritzen [993] outlines how occupancy is affected by the number of registers and the shared memory that a shader uses. Wronski [1911, 1914] discusses how the ideal occupancy rate can vary depending on the type of operations a shader performs.

着色器程序的结构是影响效率的重要特征。 一个主要因素是每个线程使用的寄存器数量。 在我们的示例中，我们假设 2000 个线程可以同时驻留在 GPU 上。 与每个线程相关联的着色器程序所需的寄存器越多，线程就越少，因此可以驻留在 GPU 中的warp也就越少。 warp的短缺可能意味着无法通过交换(swapping)来缓解停顿。 驻留的Warp被称为“飞行中(in flight)”，这个数字称为占用率。 高占用率意味着有许多 warp 可用于处理，因此空闲处理器的可能性较小。 低占用率通常会导致性能不佳。 内存获取的频率也会影响需要多少延迟隐藏(latency hiding)。 Lauritzen [993] 概述了占用率如何受寄存器数量和着色器使用的共享内存的影响。 Wronski [1911, 1914] 讨论了理想的占用率如何根据着色器执行的操作类型而变化。

Another factor affecting overall efficiency is dynamic branching, caused by “if” statements and loops. Say an “if” statement is encountered in a shader program. If all the threads evaluate and take the same branch, the warp can continue without any concern about the other branch. However, if some threads, or even one thread, take the alternate path, then the warp must execute both branches, throwing away the results not needed by each particular thread [530, 945]. This problem is called thread divergence, where a few threads may need to execute a loop iteration or perform an “if” path that the other threads in the warp do not, leaving them idle during this time.

影响整体效率的另一个因素是由“if”语句和循环引起的动态分支。 假设在着色器程序中遇到“if”语句。 如果所有线程都计算(evaluate)并采用相同的分支(take the same branch)，则 warp 可以继续而无需担心其他分支。 然而，如果某些线程，甚至一个线程采用备用路径(alternate path)，则 warp 必须执行两个分支，丢弃每个特定线程(each particular thread)不需要的结果 [530、945]。 这个问题称为线程发散(thread divergence)，其中一些线程可能需要执行循环迭代(loop iteration)或执行 warp 中其他线程不需要的“if”路径，从而使它们在这段时间内处于空闲状态。

All GPUs implement these architectural ideas, resulting in systems with strict limitations but massive amounts of compute power per watt. Understanding how this system operates will help you as a programmer make more efficient use of the power it provides. In the sections that follow we discuss how the GPU implements the rendering pipeline, how programmable shaders operate, and the evolution and function of each GPU stage.

所有 GPU 都实现了这些架构思想，从而导致系统具有严格的限制，但每瓦计算能力却很高。 理解这个系统是如何运作的将帮助您作为程序员更有效地利用它提供的功能(power)。 在接下来的部分中，我们将讨论 GPU 如何实现渲染管线、可编程着色器如何运行以及每个 GPU 阶段的演变(evolution)和功能(function)。

